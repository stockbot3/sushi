<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>Sushi - VRM Cinematic Stream</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #010103; color: #fff; overflow: hidden; height: 100vh;
        }
        #root { width: 100%; height: 100%; position: relative; }
        
        .stream-hud {
            position: absolute; top: 0; left: 0; right: 0; z-index: 100;
            background: linear-gradient(180deg, rgba(10,10,15,0.98) 0%, rgba(10,10,15,0.8) 100%);
            border-bottom: 1px solid rgba(255,255,255,0.1);
            backdrop-filter: blur(20px);
        }
        .hud-ticker {
            border-top: 1px solid rgba(255,255,255,0.06);
            padding: 6px 24px 8px;
            background: linear-gradient(135deg, rgba(255,184,28,0.06), rgba(255,107,53,0.03));
        }
        .hud-ticker-label {
            font-size: 9px; font-weight: 800; color: #FFB81C; letter-spacing: 2px; text-transform: uppercase;
        }
        .hud-ticker-text {
            font-size: 13px; color: rgba(255,255,255,0.75); line-height: 1.4;
            display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden;
        }
        .hud-top { display: flex; align-items: center; justify-content: space-between; padding: 12px 24px; }
        .hud-scores { display: flex; align-items: center; }
        .hud-team { display: flex; align-items: center; gap: 12px; padding: 6px 18px; background: rgba(255,255,255,0.05); border-radius: 12px; }
        .hud-abbr { font-size: 16px; font-weight: 900; letter-spacing: 2px; }
        .hud-pts { font-size: 40px; font-weight: 950; line-height: 1; min-width: 40px; text-align: center; }
        .hud-sep { font-size: 28px; color: rgba(255,255,255,0.15); padding: 0 15px; font-weight: 200; }
        
        .vrm-stage {
            position: absolute; inset: 0; display: flex;
            background: radial-gradient(circle at center, #1a1a2e 0%, #010103 100%);
        }
        .vrm-canvas-container { flex: 1; position: relative; }
        .vrm-canvas { width: 100%; height: 100%; display: block; }

        .subtitle-box {
            position: absolute; bottom: 40px; left: 50%; transform: translateX(-50%);
            width: 90%; max-width: 800px;
            background: rgba(5,5,10,0.9); backdrop-filter: blur(25px);
            padding: 24px 40px; border-radius: 24px;
            border: 1px solid rgba(255,255,255,0.15);
            text-align: center; z-index: 200;
            box-shadow: 0 30px 60px rgba(0,0,0,0.6);
        }
        .subtitle-name { font-size: 13px; font-weight: 900; color: #FFB81C; text-transform: uppercase; margin-bottom: 8px; letter-spacing: 3px; }
        .subtitle-text { font-size: 20px; line-height: 1.6; color: #fff; font-weight: 600; }

        .start-overlay {
            position: fixed; inset: 0; z-index: 500;
            background: rgba(0,0,0,0.95); display: flex; align-items: center; justify-content: center;
            cursor: pointer;
        }
        .start-btn {
            padding: 28px 56px; border-radius: 60px;
            background: #fff; color: #000; font-weight: 900; font-size: 22px;
            box-shadow: 0 20px 50px rgba(255,184,28,0.5);
            letter-spacing: 2px; transition: all 0.3s;
        }
    </style>
</head>
<body>
    <div id="root"></div>
    <script src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <script type="importmap"> { "imports": { "three": "https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.module.js", "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.160.0/examples/jsm/", "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3/lib/three-vrm.module.min.js" } } </script>
    <script type="module">
        import * as THREE from 'three'; import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js'; import { VRMLoaderPlugin, VRMUtils } from '@pixiv/three-vrm';
        window.THREE = THREE; window.GLTFLoader = GLTFLoader; window.VRMLoaderPlugin = VRMLoaderPlugin; window.VRMUtils = VRMUtils;
        window.modulesLoaded = true; window.dispatchEvent(new Event('modulesLoaded'));
    </script>
    <script type="text/babel">
        const { useState, useEffect, useRef, useCallback } = React;
        const DEFAULT_SAKURA_URL = 'https://raw.githubusercontent.com/pixiv/three-vrm/release/packages/three-vrm/examples/models/VRM1_Constraint_Twist_Sample.vrm';
        const DEFAULT_YUKI_URL = '/Avatar.vrm';

        const damp = (current, target, lambda, delta) => THREE.MathUtils.damp(current, target, lambda, delta);
        const noise = (t) => Math.sin(t) + Math.sin(t * 2.5) * 0.5;
        const cleanSpeech = (text) => {
            if (!text) return '';
            return text
                .replace(/^\s*(?:\d+[\).\]]|[-•])\s*/g, '')
                .replace(/(^|\n)\s*(?:\d+[\).\]]|[-•])\s*/g, '$1')
                .replace(/^\s*(?:\[?[AB]\]?[:\s—-]+)/i, '')
                .replace(/^\s*[\(\[][^)\]]+[\)\]]\s*:?\s*/g, '')
                .replace(/^\s*[A-Za-z][\w\s-]{0,20}\s*:\s*/g, (m) => {
                    const w = m.replace(':', '').trim();
                    if (w.length <= 20) return '';
                    return m;
                })
                .replace(/^["']|["']$/g, '')
                .replace(/\s*\b\d+\.\s*$/g, '')
                .trim();
        };

        const avatarEvents = {
            listeners: [],
            subscribe(fn) { this.listeners.push(fn); return () => { this.listeners = this.listeners.filter(l => l !== fn); }; },
            emit(side, type, data) { this.listeners.forEach(l => l(side, type, data)); }
        };

        class TTSService {
            constructor() { this.audioContext = null; this.isSpeaking = false; this.isMuted = false; }
            setMuted(m) { this.isMuted = !!m; }
            async init() {
                if (!this.audioContext) this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
                if (this.audioContext.state === 'suspended') {
                    try { await this.audioContext.resume(); } catch (e) {}
                }
                const b = this.audioContext.createBuffer(1, 1, 22050);
                const s = this.audioContext.createBufferSource(); s.buffer = b;
                s.connect(this.audioContext.destination); s.start(0);
            }
            async speak(text, side, voice) {
                if (this.isSpeaking) {
                    console.log('[TTS] Already speaking, skipping');
                    return;
                }
                this.isSpeaking = true;
                await this.init();
                if (this.isMuted) {
                    console.log('[TTS] Muted, skipping');
                    this.isSpeaking = false;
                    return;
                }
                try {
                    console.log('[TTS] Fetching audio:', { text: text.substring(0, 30) + '...', voice });
                    const res = await fetch('/api/tts', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ text, voice })
                    });

                    if (!res.ok) {
                        console.error('[TTS] Server error:', res.status, res.statusText);
                        throw new Error(`TTS API error: ${res.status}`);
                    }

                    const data = await res.json();
                    console.log('[TTS] Received audio:', { format: data.format, provider: data.provider, hasAudio: !!data.audio });
                    if (!data.audio) throw new Error();

                    // Detect audio format - ElevenLabs returns MP3, Piper returns WAV
                    const audioFormat = data.format || 'wav';
                    const mimeType = audioFormat === 'mp3' ? 'audio/mpeg' : 'audio/wav';

                    const bin = atob(data.audio);
                    const buf = new Uint8Array(bin.length);
                    for (let i = 0; i < bin.length; i++) buf[i] = bin.charCodeAt(i);
                    try {
                        const audioBuffer = await this.audioContext.decodeAudioData(buf.buffer);
                        console.log('[TTS] Audio decoded, duration:', audioBuffer.duration.toFixed(2) + 's');
                        const source = this.audioContext.createBufferSource();
                        source.buffer = audioBuffer;
                        const analyser = this.audioContext.createAnalyser();
                        analyser.fftSize = 256; source.connect(analyser); analyser.connect(this.audioContext.destination);
                        const dataArray = new Uint8Array(analyser.frequencyBinCount);
                        const duration = audioBuffer.duration * 1000; const start = Date.now();
                        return new Promise(resolve => {
                            let active = true;
                            const loop = () => {
                                if (!active) return;
                                if (Date.now() - start > duration) { active = false; resolve(); return; }
                                analyser.getByteFrequencyData(dataArray);
                                let avg = 0; for(let i=0; i<dataArray.length; i++) avg += dataArray[i];
                                avg /= dataArray.length;
                                avatarEvents.emit(side, 'viseme', { viseme: avg > 25 ? ['aa','ih','ou','E','oh'][Math.floor(Math.random()*5)] : 'sil', volume: avg/255 });
                                requestAnimationFrame(loop);
                            };
                            source.onended = () => { active = false; this.isSpeaking = false; avatarEvents.emit(side, 'viseme', { viseme: 'sil', volume: 0 }); console.log('[TTS] Audio ended'); resolve(); };
                            console.log('[TTS] Starting audio playback');
                            source.start(0); loop();
                        });
                    } catch (err) {
                        // iOS fallback: use HTMLAudioElement if decodeAudioData fails
                        return new Promise((resolve, reject) => {
                            const audio = new Audio(`data:${mimeType};base64,${data.audio}`);
                            audio.volume = this.isMuted ? 0 : 1;
                            let visemeTimer = null;
                            const startViseme = () => {
                                visemeTimer = setInterval(() => {
                                    avatarEvents.emit(side, 'viseme', { viseme: ['aa','ih','ou','E','oh'][Math.floor(Math.random()*5)], volume: 0.6 });
                                }, 90);
                            };
                            const stopViseme = () => {
                                if (visemeTimer) clearInterval(visemeTimer);
                                avatarEvents.emit(side, 'viseme', { viseme: 'sil', volume: 0 });
                            };
                            audio.onended = () => { stopViseme(); this.isSpeaking = false; resolve(); };
                            audio.onerror = () => { stopViseme(); this.isSpeaking = false; reject(); };
                            const playPromise = audio.play();
                            startViseme();
                            if (playPromise && playPromise.catch) {
                                playPromise.catch(() => {
                                    stopViseme();
                                    this.isSpeaking = false;
                                    reject();
                                });
                            }
                        }).catch(() => {
                            this.isSpeaking = false;
                            return new Promise(r => {
                                const u = new SpeechSynthesisUtterance(text);
                                u.onend = r; window.speechSynthesis.speak(u);
                            });
                        });
                    }
                } catch (e) {
                    this.isSpeaking = false;
                    return new Promise(r => {
                        const u = new SpeechSynthesisUtterance(text);
                        u.onend = r; window.speechSynthesis.speak(u);
                    });
                }
            }
        }

        function Avatar({ side, modelUrl, activeSpeaker }) {
            const canvasRef = useRef(null), vrmRef = useRef(null), speakerRef = useRef(activeSpeaker), isLoadedRef = useRef(false);
            const basePoseRef = useRef(null);
            const blinkTimerRef = useRef(null);
            useEffect(() => { speakerRef.current = activeSpeaker; }, [activeSpeaker]);

            useEffect(() => {
                basePoseRef.current = null;
                let mounted = true;
                const init = () => {
                    const { THREE, GLTFLoader, VRMLoaderPlugin, VRMUtils } = window;
                    const scene = new THREE.Scene(), cam = new THREE.PerspectiveCamera(30, canvasRef.current.clientWidth / canvasRef.current.clientHeight, 0.1, 1000);
                    cam.position.set(0, 1.35, 2.2); cam.lookAt(0, 1.25, 0);
                    const ren = new THREE.WebGLRenderer({ canvas: canvasRef.current, alpha: true, antialias: true });
                    ren.setSize(canvasRef.current.clientWidth, canvasRef.current.clientHeight); ren.setPixelRatio(window.devicePixelRatio);
                    scene.add(new THREE.AmbientLight(0xffffff, 0.4));
                    const key = new THREE.DirectionalLight(0xffeeb1, 1.5); key.position.set(-1, 2, 3); scene.add(key);
                    const rim = new THREE.DirectionalLight(0x4455ff, 2.0); rim.position.set(2, 2, -2); scene.add(rim);

                    const scheduleBlink = () => {
                        if (!mounted || !vrmRef.current?.expressionManager) return;
                        const m = vrmRef.current.expressionManager;
                        const delay = 2000 + Math.random() * 3500;
                        blinkTimerRef.current = setTimeout(() => {
                            m.setValue('blink', 1);
                            setTimeout(() => m.setValue('blink', 0), 140);
                            scheduleBlink();
                        }, delay);
                    };

                    const applyBasePose = (humanoid) => {
                        if (!humanoid || basePoseRef.current) return;
                        const bones = {
                            spine: humanoid.getNormalizedBoneNode('spine'),
                            chest: humanoid.getNormalizedBoneNode('chest'),
                            head: humanoid.getNormalizedBoneNode('head'),
                            lua: humanoid.getNormalizedBoneNode('leftUpperArm'),
                            rua: humanoid.getNormalizedBoneNode('rightUpperArm'),
                            lla: humanoid.getNormalizedBoneNode('leftLowerArm'),
                            rla: humanoid.getNormalizedBoneNode('rightLowerArm'),
                            lhand: humanoid.getNormalizedBoneNode('leftHand'),
                            rhand: humanoid.getNormalizedBoneNode('rightHand'),
                            lshoulder: humanoid.getNormalizedBoneNode('leftShoulder'),
                            rshoulder: humanoid.getNormalizedBoneNode('rightShoulder')
                        };
                        // Natural resting pose - arms hanging naturally at sides
                        if (bones.lshoulder) bones.lshoulder.rotation.z = 0;
                        if (bones.rshoulder) bones.rshoulder.rotation.z = 0;
                        // Upper arms: straight down with slight forward rotation
                        if (bones.lua) { bones.lua.rotation.z = 0; bones.lua.rotation.x = 0.1; bones.lua.rotation.y = 0.05; }
                        if (bones.rua) { bones.rua.rotation.z = 0; bones.rua.rotation.x = 0.1; bones.rua.rotation.y = -0.05; }
                        // Lower arms: slight natural bend at elbow
                        if (bones.lla) { bones.lla.rotation.x = -0.2; bones.lla.rotation.y = 0; bones.lla.rotation.z = 0; }
                        if (bones.rla) { bones.rla.rotation.x = -0.2; bones.rla.rotation.y = 0; bones.rla.rotation.z = 0; }
                        // Hands: relaxed, slightly cupped
                        if (bones.lhand) { bones.lhand.rotation.z = 0.05; bones.lhand.rotation.x = -0.05; }
                        if (bones.rhand) { bones.rhand.rotation.z = -0.05; bones.rhand.rotation.x = -0.05; }
                        if (bones.spine) bones.spine.rotation.x = 0.02;
                        if (bones.chest) bones.chest.rotation.x = 0.01;

                        const rot = (b) => b ? { x: b.rotation.x, y: b.rotation.y, z: b.rotation.z } : null;
                        basePoseRef.current = {
                            spine: rot(bones.spine),
                            chest: rot(bones.chest),
                            head: rot(bones.head),
                            lua: rot(bones.lua),
                            rua: rot(bones.rua),
                            lla: rot(bones.lla),
                            rla: rot(bones.rla),
                            lhand: rot(bones.lhand),
                            rhand: rot(bones.rhand)
                        };
                    };

                    new GLTFLoader().register(p => new VRMLoaderPlugin(p)).load(modelUrl || (side === 'left' ? DEFAULT_SAKURA_URL : DEFAULT_YUKI_URL), gltf => {
                        if (!mounted) return;
                        const vrm = gltf.userData.vrm;
                        if (vrm) {
                            vrmRef.current = vrm; VRMUtils.rotateVRM0(vrm); scene.add(vrm.scene);
                            if (vrm.humanoid) applyBasePose(vrm.humanoid);
                        } else { scene.add(gltf.scene); }
                        isLoadedRef.current = true;
                        scheduleBlink();
                    });

                    const clock = new THREE.Clock();
                    const animate = () => {
                        if (!mounted) return; requestAnimationFrame(animate);
                        const dt = Math.min(clock.getDelta(), 0.05), time = clock.getElapsedTime();
                        if (vrmRef.current && isLoadedRef.current) {
                            vrmRef.current.update(dt);
                            const h = vrmRef.current.humanoid;
                            if (h && basePoseRef.current) {
                                const spine = h.getNormalizedBoneNode('spine'),
                                    chest = h.getNormalizedBoneNode('chest'),
                                    head = h.getNormalizedBoneNode('head'),
                                    lua = h.getNormalizedBoneNode('leftUpperArm'),
                                    rua = h.getNormalizedBoneNode('rightUpperArm'),
                                    lla = h.getNormalizedBoneNode('leftLowerArm'),
                                    rla = h.getNormalizedBoneNode('rightLowerArm'),
                                    lhand = h.getNormalizedBoneNode('leftHand'),
                                    rhand = h.getNormalizedBoneNode('rightHand');

                                const isSpeaking = speakerRef.current === side;

                                // Base animation layers
                                const sway = noise(time * 0.35) * 0.02;
                                const breath = Math.sin(time * 1.2) * 0.015;
                                const breathSlow = Math.sin(time * 0.7) * 0.01;

                                // Speech-driven gesture system with multiple patterns
                                const gesturePhase = time * 1.8;
                                const emphasisCycle = Math.floor(time / 3) % 3; // Cycle through 3 gesture types

                                let gestureL = { shoulder: 0, elbow: 0, hand: 0, lift: 0 };
                                let gestureR = { shoulder: 0, elbow: 0, hand: 0, lift: 0 };

                                if (isSpeaking) {
                                    // Alternate between different gesture patterns for variety
                                    if (emphasisCycle === 0) {
                                        // Pattern 1: Open gestures (explaining) - arms out and bent
                                        const wave = Math.sin(gesturePhase);
                                        gestureL = { shoulder: 0.4 + wave * 0.2, elbow: -0.8 + wave * 0.15, hand: wave * 0.08, lift: 0 };
                                        gestureR = { shoulder: -0.4 - wave * 0.2, elbow: -0.8 - wave * 0.15, hand: -wave * 0.08, lift: 0 };
                                    } else if (emphasisCycle === 1) {
                                        // Pattern 2: Emphatic (one hand up, one relaxed)
                                        const alt = Math.sin(gesturePhase * 1.3);
                                        gestureL = { shoulder: 0.5 + alt * 0.25, elbow: -0.9 + alt * 0.2, hand: alt * 0.1, lift: 0 };
                                        gestureR = { shoulder: -0.2 - alt * 0.1, elbow: -0.5 - alt * 0.12, hand: -alt * 0.05, lift: 0 };
                                    } else {
                                        // Pattern 3: Conversational (natural, alternating)
                                        const subtle = Math.sin(gesturePhase * 0.9);
                                        const noise1 = noise(time * 0.8) * 0.1;
                                        const noise2 = noise(time * 0.8 + 100) * 0.1;
                                        gestureL = { shoulder: 0.3 + subtle * 0.15 + noise1, elbow: -0.65 + subtle * 0.18, hand: subtle * 0.06, lift: 0 };
                                        gestureR = { shoulder: -0.3 - subtle * 0.15 + noise2, elbow: -0.65 - subtle * 0.18, hand: -subtle * 0.06, lift: 0 };
                                    }
                                } else {
                                    // Idle: arms straight down at sides with minimal movement
                                    const idleWave = noise(time * 0.4) * 0.03;
                                    const idleWave2 = noise(time * 0.4 + 50) * 0.03;
                                    gestureL = { shoulder: idleWave, elbow: -0.05 + breathSlow, hand: 0, lift: 0 };
                                    gestureR = { shoulder: idleWave2, elbow: -0.05 - breathSlow, hand: 0, lift: 0 };
                                }

                                // Apply animations with smooth damping
                                if (spine && basePoseRef.current.spine) {
                                    spine.rotation.x = damp(spine.rotation.x, basePoseRef.current.spine.x + breath * 0.6 + (isSpeaking ? 0.04 : 0), 3, dt);
                                    spine.rotation.y = damp(spine.rotation.y, basePoseRef.current.spine.y + sway * 0.3, 3, dt);
                                    spine.rotation.z = damp(spine.rotation.z, basePoseRef.current.spine.z + sway * 0.2, 3, dt);
                                    spine.position.y = breathSlow * 0.5;
                                }

                                if (chest && basePoseRef.current.chest) {
                                    chest.rotation.x = damp(chest.rotation.x, basePoseRef.current.chest.x + breath * 0.4 + (isSpeaking ? 0.03 : 0), 3, dt);
                                    chest.rotation.y = damp(chest.rotation.y, basePoseRef.current.chest.y + sway * 0.15, 4, dt);
                                }

                                if (head && basePoseRef.current.head) {
                                    const headTurn = isSpeaking ? Math.sin(time * 1.8) * 0.08 : noise(time * 0.5) * 0.04;
                                    const headNod = isSpeaking ? Math.sin(time * 2.2) * 0.06 : Math.sin(time * 0.9) * 0.012;
                                    const headTilt = isSpeaking ? Math.sin(time * 1.5) * 0.04 : noise(time * 0.6 + 20) * 0.02;

                                    head.rotation.y = damp(head.rotation.y, basePoseRef.current.head.y + headTurn + sway * 0.2, 4, dt);
                                    head.rotation.x = damp(head.rotation.x, basePoseRef.current.head.x + headNod, 4, dt);
                                    head.rotation.z = damp(head.rotation.z, basePoseRef.current.head.z + headTilt, 4, dt);
                                }

                                // Left arm - natural bending with shoulder, elbow, hand coordination
                                if (lua && basePoseRef.current.lua) {
                                    lua.rotation.z = damp(lua.rotation.z, basePoseRef.current.lua.z + gestureL.shoulder, 3.5, dt);
                                    lua.rotation.x = damp(lua.rotation.x, basePoseRef.current.lua.x - 0.1 + gestureL.lift, 3.5, dt);
                                    lua.rotation.y = damp(lua.rotation.y, basePoseRef.current.lua.y + sway * 0.3, 4, dt);
                                }
                                if (lla && basePoseRef.current.lla) {
                                    lla.rotation.x = damp(lla.rotation.x, basePoseRef.current.lla.x + gestureL.elbow, 3, dt);
                                    lla.rotation.z = damp(lla.rotation.z, basePoseRef.current.lla.z + gestureL.hand * 0.3, 4, dt);
                                }
                                if (lhand && basePoseRef.current.lhand) {
                                    lhand.rotation.z = damp(lhand.rotation.z, basePoseRef.current.lhand.z + gestureL.hand, 4.5, dt);
                                    lhand.rotation.x = damp(lhand.rotation.x, basePoseRef.current.lhand.x + gestureL.hand * 0.2, 5, dt);
                                }

                                // Right arm - mirror with slight offset for asymmetry
                                if (rua && basePoseRef.current.rua) {
                                    rua.rotation.z = damp(rua.rotation.z, basePoseRef.current.rua.z + gestureR.shoulder, 3.5, dt);
                                    rua.rotation.x = damp(rua.rotation.x, basePoseRef.current.rua.x - 0.1 + gestureR.lift, 3.5, dt);
                                    rua.rotation.y = damp(rua.rotation.y, basePoseRef.current.rua.y - sway * 0.3, 4, dt);
                                }
                                if (rla && basePoseRef.current.rla) {
                                    rla.rotation.x = damp(rla.rotation.x, basePoseRef.current.rla.x + gestureR.elbow, 3, dt);
                                    rla.rotation.z = damp(rla.rotation.z, basePoseRef.current.rla.z + gestureR.hand * 0.3, 4, dt);
                                }
                                if (rhand && basePoseRef.current.rhand) {
                                    rhand.rotation.z = damp(rhand.rotation.z, basePoseRef.current.rhand.z + gestureR.hand, 4.5, dt);
                                    rhand.rotation.x = damp(rhand.rotation.x, basePoseRef.current.rhand.x + gestureR.hand * 0.2, 5, dt);
                                }
                            }
                        }
                        ren.render(scene, cam);
                    };
                    animate();
                };
                if (window.modulesLoaded) init(); else window.addEventListener('modulesLoaded', init);
                return () => {
                    mounted = false;
                    if (blinkTimerRef.current) clearTimeout(blinkTimerRef.current);
                };
            }, [side, modelUrl]);

            useEffect(() => {
                return avatarEvents.subscribe((targetSide, type, data) => {
                    if (targetSide !== side || !vrmRef.current?.expressionManager) return;
                    const m = vrmRef.current.expressionManager, map = m._expressionMap;
                    if (type === 'viseme') {
                        ['aa','ih','ou','E','oh','a','i','u','e','o'].forEach(v => { if (v in map) m.setValue(v, 0); });
                        if (data.viseme !== 'sil') {
                            let t = data.viseme;
                            if (!(t in map)) { const f = { 'aa':'a','ih':'i','ou':'u','E':'e','oh':'o' }; t = f[data.viseme] || data.viseme[0].toLowerCase(); }
                            if (!(t in map)) t = t.toUpperCase();
                            if (t in map) m.setValue(t, data.volume * 2.0);
                        }
                        m.update();
                    }
                });
            }, [side]);

            return <div className="vrm-canvas-container"><canvas ref={canvasRef} className="vrm-canvas" /></div>;
        }

        function App() {
            const [started, setStarted] = useState(false), [game, setGame] = useState(null), [commentary, setCommentary] = useState(null);
            const [activeSpeaker, setActiveSpeaker] = useState(null), [sub, setSub] = useState({ name: "", text: "" });
            const [sessionMeta, setSessionMeta] = useState(null);
            const [muted, setMuted] = useState(false);
            const voiceRef = useRef(new TTSService()), queueRef = useRef([]), isProcessingRef = useRef(false), voicedRef = useRef(new Set()), lastSeqRef = useRef(-1);

            useEffect(() => {
                const sid = new URLSearchParams(window.location.search).get('session');
                if (!started || !sid) return;
                fetch('/api/sessions').then(r => r.json()).then(sessions => {
                    const s = sessions.find(s => s.id === sid); if (s) setSessionMeta(s);
                });

                // Send heartbeat ping every 30 seconds to indicate active viewer
                const sendHeartbeat = () => {
                    fetch(`/api/sessions/${sid}/ping`, { method: 'POST' })
                        .then(r => r.json())
                        .then(data => console.log('[Heartbeat] Sent', data))
                        .catch(e => console.error('[Heartbeat] Error:', e));
                };
                sendHeartbeat(); // Send immediately
                const heartbeatInterval = setInterval(sendHeartbeat, 30000); // Every 30s

                const poll = async () => {
                    try {
                        const [g, c] = await Promise.all([ fetch(`/api/sessions/${sid}/game`).then(r => r.json()), fetch(`/api/sessions/${sid}/commentary/latest`).then(r => r.json()) ]);
                        setGame(g);
                        if (c && Array.isArray(c.turns)) {
                            const seq = c.timestamp;
                            if (seq !== lastSeqRef.current) {
                                lastSeqRef.current = seq;
                                setCommentary(c);
                                queueRef.current = [...c.turns];
                                processQueue();
                            }
                        }
                    } catch (e) {}
                };
                poll(); const itv = setInterval(poll, 8000);
                return () => {
                    clearInterval(itv);
                    clearInterval(heartbeatInterval);
                };
            }, [started]);

            const processQueue = async () => {
                if (isProcessingRef.current || queueRef.current.length === 0) return;
                isProcessingRef.current = true;
                while (queueRef.current.length > 0) {
                    const t = queueRef.current.shift();
                    const side = t.speaker === 'A' ? 'left' : 'right';
                    const cMeta = sessionMeta?.commentators?.[t.speaker === 'A' ? 0 : 1];
                    const defaultVoice = side === 'left' ? 'rachel' : 'adam';
                    const voiceToUse = cMeta?.voice || defaultVoice;
                    const cleaned = cleanSpeech(t.text);

                    console.log(`[TTS] Speaking:`, { side, voice: voiceToUse, text: cleaned.substring(0, 50) + '...' });

                    setSub({ name: t.name, text: cleaned });
                    setActiveSpeaker(side);

                    try {
                        await voiceRef.current.speak(cleaned, side, voiceToUse);
                    } catch (err) {
                        console.error('[TTS] Error:', err);
                    }

                    setActiveSpeaker(null);
                    setSub({ name: "", text: "" });
                    await new Promise(r => setTimeout(r, 1000));
                }
                isProcessingRef.current = false;
            };

            useEffect(() => {
                voiceRef.current.setMuted(muted);
            }, [muted]);

            if (!started) return <div className="start-overlay" onClick={async () => { await voiceRef.current.init(); setStarted(true); }}><div className="start-btn">TAP TO ENTER STREAM</div></div>;

            const cA = sessionMeta?.commentators?.[0], cB = sessionMeta?.commentators?.[1];
            const recentText = (() => {
                if (commentary?.play?.description) return commentary.play.description;
                if (game?.situation?.lastPlay) return game.situation.lastPlay;
                if (game?.status?.detail) return `Status: ${game.status.detail}`;
                return 'Waiting for live updates...';
            })();
            return (
                <div id="root">
                <div className="stream-hud">
                    <div className="hud-top">
                            <div className="hud-scores">
                                <div className="hud-team"><span className="hud-abbr" style={{color: game?.away?.color}}>{game?.away?.abbreviation}</span><span className="hud-pts">{game?.away?.score}</span></div>
                                <span className="hud-sep">:</span>
                                <div className="hud-team"><span className="hud-pts">{game?.home?.score}</span><span className="hud-abbr" style={{color: game?.home?.color}}>{game?.home?.abbreviation}</span></div>
                            </div>
                    </div>
                    <div className="hud-ticker">
                        <div className="hud-ticker-label">Recent Play</div>
                        <div className="hud-ticker-text">{recentText}</div>
                    </div>
                </div>
                <button
                    onClick={async () => { setMuted(m => !m); if (muted) { await voiceRef.current.init(); } }}
                    style={{
                        position: 'absolute', right: 16, top: 86, zIndex: 150,
                        padding: '8px 12px', borderRadius: 10, border: '1px solid rgba(255,255,255,0.15)',
                        background: muted ? 'rgba(239,68,68,0.25)' : 'rgba(255,255,255,0.08)',
                        color: '#fff', fontWeight: 800, fontSize: 10, letterSpacing: 1
                    }}
                >
                    {muted ? 'UNMUTE' : 'MUTE'}
                </button>
                <div className="vrm-stage">
                        <Avatar side="left" modelUrl={cA?.avatarUrl} activeSpeaker={activeSpeaker} />
                        <Avatar side="right" modelUrl={cB?.avatarUrl} activeSpeaker={activeSpeaker} />
                    </div>
                    {sub.text && ( <div className="subtitle-box"> <div className="subtitle-name">{sub.name}</div> <div className="subtitle-text">{sub.text}</div> </div> )}
                </div>
            );
        }
        ReactDOM.createRoot(document.getElementById('root')).render(<App />);
    </script>
</body>
</html>
